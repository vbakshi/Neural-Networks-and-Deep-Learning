{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## SVM vs Neural Networks\n",
    "* 1. SVM gives a maximum margin hyper-plane whereas Neural Networks gives a boundary such that the loss is minimized which may not be a maximum margin boundary. 2. SGD or ADAM implemented as opitimization algorithms in NN may get stuck at a local minima whereas SVM objective is convex and optimization yields a global minima. **\n",
    "   \n",
    "\n",
    "\n",
    "2) Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: **The advantage of using relu-activation function lies in two aspects: The first is to solve the so called “exploding/vanishing gradient”. The gradient has a constant value when the neuron is active which accelerates the convergence speed. Therefore computer vision which requires deep neural networks, as the number of layers increases, the gradient does not vanish or explode (since it is = 1) unlike sigmoid for which the gradient is always < 1 and causes the gradient to become close to 0 as we backpropagate.**\n",
    "   \n",
    "   **Another benefit of ReLUs is sparsity. Sparsity arises when relu(a) such that a < 0. The more such units that exist in a layer the more sparse the resulting representation. Sigmoids on the other hand are always likely to generate some non-zero value resulting in dense representations. Sparse representations seem to be more beneficial than dense representations.**\n",
    "   \n",
    "\n",
    "3) Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  the hyperparameters, which stategies you did you use to improve the network, show the results of intermediate and final steps.\n",
    "\n",
    "   Your answer: **My best model in the two layer neural network yielded test-set accuracy of 0.531 with the following hyper-parameters:\n",
    "   hidden layer unit = 250, regularization = 0.25, weight_scale = 1e-3, number of epochs = 20, learning rate = 1e-3 and batch size = 500**\n",
    "   \n",
    "   ** I started with the base model given to us, hidden layer unit = 100, regularization = 0.25, weight_scale = 1e-1, number of epochs = 10, learning rate = 1e-3 and batch size = 500**\n",
    "   \n",
    "   ** Step - 1, Epoch number increased: I tried increasing the epochs to 20 to check if the NN required more epochs to train. I obtained a test set accuracy of 0.22 **\n",
    "   \n",
    "   \n",
    "   ** Step - 2, Hidden Layer Size increased: Next I increased the number of units in the hidden layer to 250 and observed a test set accuracy of 0.26 **\n",
    "   \n",
    "   ** Step - 3, Regularization decreased: When I looked at the train and validation plot I observed that the model was a simple model and the regularization could be decrease which did not improve the accuracy by a lot and it remained close to the same value of 0.26, so I retained the same regularization.**\n",
    "   \n",
    "   ** Step - 4, Weight Scale: I changed the weight scale to 1e-2 and then to 1e-3. This improved my accuracy by almost 0.2 to 0.49**\n",
    "    \n",
    "** Step - 5, Traning strategy: I changed the training strategy to pick random samples over orderly selection which slightly imporoved the accuracy to 0.51**\n",
    "    \n",
    " ** Step - 6, SGD with momentum: Finally I used SGD with momentum which caused the accuracy to increase by nearly  to 0.021 to 0.531 **\n",
    " \n",
    "    \n",
    "  ** Since I obtained an accuracy of 0.531, I did not try to incorporate PCA and saved this as my best model**\n",
    " \n",
    "   \n",
    "\n",
    "4) **Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation** if you want to use it to find a best set of hyperparameter of the **Linear SVM classification** problem.\n",
    "\n",
    "   Your answer: **In cross-validation, the data is  split repeatedly and multiple models are trained. The most commonly used version of cross-validation is k-fold cross-validation, where k is a user-specified number, usually 5 or 10. When performing k-fold cross-validation, the data is first partitioned into k parts of (approximately) equal size, called folds. Next, a sequence of models is trained. The first model is trained using the first fold as the test set, and the remaining folds (2–5) are used as the training set. The model is built using the data in folds 2–5, and then the accuracy is evaluated on fold 1. Then another model is built, this time using fold 2 as the test set and the data in folds 1, 3, 4, and 5 as the training set. This process is repeated using folds 3, 4, and 5 as test sets. For each of these five splits of the data into training and test sets, we compute the accuracy. In the end, we have collected five accuracy values. The mean of the accuracies is a metric which gives you a performance metric for the model for a specific set of hyper-paramters. \n",
    "   For SVM we create a combination of values for the hyper-parameters C (regularization coefficient) and epsilon(tolerance) and obtain the mean k-fold cross-validation accuracy for each combination of C and epsilon. Finally the combination which gives the best mean k-fold cross valication accuracy are selected as the best hyper-parameters. Since we are using the k-fold cross validation score, the model is generalizable.**\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
